from pathlib import Path
import sys

import textarena as ta
import json
import os
import argparse
import logging
import time
from typing import Dict, List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from threading import Lock
from utils.utils import start_vllm_server, stop_vllm_server
import re
import random

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename=f'logs/learnarena_benchmark_{time.strftime("%Y%m%d_%H%M%S")}.log'
)
logger = logging.getLogger(__name__)

class LearnArenaBenchmark:
    """
    LearnArena: A benchmark suite for evaluating general learning ability across three dimensions:
    1. Learning from Instructor: Player-0 provides suggestions for improvement
    2. Learning from Concept: Concise game rule summaries generated by Qwen2.5-32B
    3. Learning from Experience: Player-1 analyzes past experiences and applies conclusions
    """
    
    def __init__(self, player0_model: str = "qwen2.5-32b-chat", player0_port: int = 8000, 
                 player1_port: int = 8001):
        self.player0_model = player0_model
        self.player0_port = player0_port
        self.player1_port = player1_port
        self.game_summaries = {}  # Cache for game summaries
        
    def get_agent(self, model_name: str, port: int):
        """Create an agent with specified model and port"""
        agent = ta.agents.OpenRouterAgent(
            model_name=model_name,
            api_base=f"http://localhost:{port}/v1",
            api_key="your_api_key_here",
            timeout=120
        )
        
        # Wrap agent call for better error handling
        original_call = agent.__call__
        def safe_call(observation: str) -> str:
            try:
                response = original_call(observation)
                if response is None:
                    logger.error(f"Agent {model_name} returned None response")
                    return "Error: No response generated"
                return response
            except Exception as e:
                logger.error(f"Error in {model_name} agent call: {type(e).__name__}: {str(e)}")
                return f"Error: {type(e).__name__}: {str(e)}"
        
        agent.__call__ = safe_call
        return agent
    
    def get_game_summary(self, game: str, agent, env) -> str:
        """Generate or load a summary of the game rules with strategic advice for winning"""
        summary_dir = "environment_summary"
        summary_file = os.path.join(summary_dir, f"{game}.jsonl")
        
        # Check if summary already exists
        if os.path.exists(summary_file):
            logger.info(f"Loading existing game summary for {game}")
            try:
                with open(summary_file, 'r') as f:
                    summary_data = json.load(f)
                    return summary_data.get("summary", "")
            except Exception as e:
                logger.error(f"Error loading game summary: {str(e)}")
        
        # Generate new summary
        logger.info(f"Generating new game summary for {game}")
        
        # Reset environment to get initial observation
        env.reset(num_players=2)
        player_id, observation = env.get_observation()
        
        # Save original system prompt
        original_system_prompt = agent.system_prompt
        
        # Set system prompt for summary generation
        summary_prompt = ("You are an expert game strategist with deep knowledge of game theory and optimal play. "
                         "Your task is to provide concise, actionable strategic advice that will help a player win. "
                         "Focus on identifying winning patterns, key decision points, and optimal strategies.")
        agent.system_prompt = summary_prompt
        
        # Generate the summary
        prompt = (
            f"For this game '{game}', provide very brief winning strategies based on this initial observation. "
            f"Don't explain rules in detail.\n\n"
            f"WINNING STRATEGIES:\n"
            f"- Top 3-5 strategic principles that lead to victory\n"
            f"- Best opening moves or early game tactics\n"
            f"- Key patterns to recognize during gameplay\n" 
            f"- Critical mistakes to avoid\n\n"
            f"Game observation: {observation}\n\n"
            f"Keep your response concise and focused on practical advice that will maximize winning chances."
        )
        
        try:
            summary = agent(prompt)
            
            # Create directory and save summary
            os.makedirs(summary_dir, exist_ok=True)
            with open(summary_file, 'w') as f:
                json.dump({"game": game, "summary": summary}, f)
                
            logger.info(f"Saved game summary for {game}")
        except Exception as e:
            logger.error(f"Error generating game summary: {str(e)}")
            summary = f"Error generating summary: {str(e)}"
        
        # Restore original system prompt
        agent.system_prompt = original_system_prompt
        env.close()
        
        return summary
    
    def select_top_experiences(self, game_experiences: List[Dict], k: int = 3) -> List[Dict]:
        """
        Select top k experiences from previous games based on game quality scores.
        Player-1 selects three experiences from the previous k-1 games.
        """
        if len(game_experiences) < k:
            return game_experiences
        
        # Sort by score (higher is better) and return top k
        sorted_experiences = sorted(game_experiences, key=lambda x: x.get('score', 0), reverse=True)
        return sorted_experiences[:k]
    
    def generate_instructor_feedback(self, player0_agent, game_history: Dict, game_outcome: str) -> str:
        """
        Learning from Instructor: Player-0 provides suggestions for improvement.
        """
        if not game_history.get("moves"):
            return "No game moves to analyze"
        
        last_move = game_history["moves"][-1]
        
        # Save original system prompt
        original_prompt = player0_agent.system_prompt
        
        try:
            # Set system prompt for instructor role
            player0_agent.system_prompt = ("You are an expert game instructor. Analyze the game and provide "
                                         "constructive feedback and strategic advice to help the player improve. "
                                         "Keep your advice concise and actionable, under 300 words.")
            
            feedback_prompt = (
                f"As an expert instructor, analyze this game and provide strategic advice for improvement.\n"
                f"Game outcome: {game_outcome}\n"
                f"Final observation: {last_move['observation']}\n"
                f"Final action: {last_move['action']}\n\n"
                f"Provide feedback in this format:\n"
                f"1. Key strengths in the gameplay\n"
                f"2. Areas for improvement\n"
                f"3. Specific strategic advice\n"
                f"4. Tactical recommendations for future games\n\n"
                f"Focus on actionable insights that will lead to better performance."
            )
            
            feedback = player0_agent(feedback_prompt)
            if feedback is None or "Error:" in feedback:
                logger.error(f"Error getting instructor feedback: {feedback}")
                return "Error generating instructor feedback"
            
            return feedback
            
        except Exception as e:
            logger.error(f"Exception getting instructor feedback: {type(e).__name__}: {str(e)}")
            return f"Error: {type(e).__name__}: {str(e)}"
        finally:
            # Restore original system prompt
            player0_agent.system_prompt = original_prompt
    
    def generate_experience_analysis(self, player1_agent, selected_experiences: List[Dict]) -> str:
        """
        Learning from Experience: Player-1 analyzes selected past experiences and draws conclusions.
        """
        if not selected_experiences:
            return ""
        
        # Save original system prompt
        original_prompt = player1_agent.system_prompt
        
        try:
            # Set system prompt for self-analysis
            player1_agent.system_prompt = ("You are a player analyzing your own gameplay. "
                                           "Your task is to identify strengths, weaknesses, "
                                           "and learn from this experience to enhance future performance.")
            
            analysis_prompt = "Analyze this history and provide strategic advice.\n\n"
            analysis_prompt += "Game history: "
            for i, exp in enumerate(selected_experiences, 1):
                outcome = exp.get('outcome', 'Unknown')
                score = exp.get('score', 0)
                analysis_prompt += f"Experience #{i} (Score: {score}/10, Outcome: {outcome}):\n"
                
                if 'instructor_feedback' in exp:
                    analysis_prompt += f"Instructor feedback: {exp['instructor_feedback']}\n"
                
                if 'moves' in exp and exp['moves']:
                    last_move = exp['moves'][-1]
                    analysis_prompt += f"Last move: {last_move.get('action', 'N/A')}\n"
                    analysis_prompt += f"Last observation: {last_move.get('observation', 'N/A')}\n"
                
                analysis_prompt += "\n"
            
            analysis_prompt += ("Provide your analysis in this format:\n"
                              "1. Strategic principles to follow\n"
                              "2. Specific moves to consider\n"
                              "3. Moves to avoid\n"
                              "4. Key patterns to watch for\n\n")
            
            analysis = player1_agent(analysis_prompt)
            if analysis is None or "Error:" in analysis:
                logger.error(f"Error getting experience analysis: {analysis}")
                return "Error generating experience analysis"
            
            return analysis
            
        except Exception as e:
            logger.error(f"Exception getting experience analysis: {type(e).__name__}: {str(e)}")
            return f"Error: {type(e).__name__}: {str(e)}"
        finally:
            # Restore original system prompt
            player1_agent.system_prompt = original_prompt
    
    def combine_instructor_feedback(self, player0_agent, selected_experiences: List[Dict]) -> str:
        """
        Combine instructor feedback from top selected experiences into a coherent strategic guide.
        """
        if not selected_experiences:
            return ""
        
        # Save original system prompt
        original_prompt = player0_agent.system_prompt
        
        try:
            # Set system prompt for combining feedback
            player0_agent.system_prompt = ("You are an expert game instructor. "
                                           "Your task is to provide detailed strategic "
                                           "advice for the game based on the provided information.")
            
            # Create prompt for combining feedback
            combine_prompt = "Analyze this history and provide strategic advice.\n\n"
            combine_prompt += "Game history: "
            
            for i, exp in enumerate(selected_experiences, 1):
                outcome = exp.get('outcome', 'Unknown')
                score = exp.get('score', 0)
                combine_prompt += f"Experience #{i} (Score: {score}/10, Outcome: {outcome}):\n"
                
                if 'moves' in exp and exp['moves']:
                    last_move = exp['moves'][-1]
                    combine_prompt += f"Last move: {last_move.get('action', 'N/A')}\n"
                    combine_prompt += f"Last observation: {last_move.get('observation', 'N/A')}\n"
                
                combine_prompt += "\n"

            
            combine_prompt += ("Provide your analysis in this format:\n"
                              "1. Strategic principles to follow\n"
                              "2. Specific moves to consider\n"
                              "3. Moves to avoid\n"
                              "4. Key patterns to watch for\n\n")
            
            # Get combined feedback
            combined_feedback = player0_agent(combine_prompt)
            if combined_feedback is None or "Error:" in combined_feedback:
                logger.error(f"Error combining instructor feedback: {combined_feedback}")
                return ""
            
            return combined_feedback
            
        except Exception as e:
            logger.error(f"Exception combining instructor feedback: {type(e).__name__}: {str(e)}")
            return ""
        finally:
            # Restore original system prompt
            player0_agent.system_prompt = original_prompt
    
    def score_game_quality(self, player0_agent, game_history: Dict, game_outcome: str) -> int:
        """Score the game quality on a scale of 0-10 using Player-0 as judge"""
        if not game_history.get("moves"):
            return 0
        
        last_move = game_history["moves"][-1]
        
        # Save original system prompt
        original_prompt = player0_agent.system_prompt
        
        try:
            # Set system prompt for scoring
            player0_agent.system_prompt = ("You are an expert game judge. Evaluate game quality objectively "
                                         "on a scale of 0-10 based on strategic depth, tactical execution, "
                                         "and overall gameplay quality.")
            
            scoring_prompt = (
                f"Score this game on a scale of 0-10 based on overall gameplay quality.\n"
                f"Game outcome: {game_outcome}\n"
                f"Final observation: {last_move['observation']}\n"
                f"Final action: {last_move['action']}\n\n"
                f"Rating scale:\n"
                f"0-2: Poor - Basic mistakes, no clear strategy\n"
                f"3-4: Fair - Some good moves but inconsistent play\n"
                f"5-6: Good - Solid strategic play with clear planning\n"
                f"7-8: Very Good - Strong tactical execution and strategy\n"
                f"9-10: Excellent - Masterful play with optimal decisions\n\n"
                f"End your response with 'Score: X/10' where X is your rating."
            )
            
            score_response = player0_agent(scoring_prompt)
            if score_response is None or "Error:" in score_response:
                logger.error(f"Error getting game score: {score_response}")
                return 0
            
            # Extract score using regex
            match = re.search(r'(\d+)/10', score_response)
            if match:
                return int(match.group(1))
            else:
                logger.warning(f"Could not extract score from response: {score_response}")
                return 0
                
        except Exception as e:
            logger.error(f"Exception getting game score: {type(e).__name__}: {str(e)}")
            return 0
        finally:
            # Restore original system prompt
            player0_agent.system_prompt = original_prompt
    
    def run_single_game(self, game: str, player1_model: str, game_round: int, 
                       game_experiences: List[Dict], learning_enabled: bool = True) -> Dict:
        """
        Run a single game between Player-0 (Qwen2.5-32B) and Player-1 (evaluated model).
        
        Args:
            game: Game environment ID
            player1_model: Model name for Player-1
            game_round: Current round number
            game_experiences: Previous game experiences for learning
            learning_enabled: Whether to apply learning from previous experiences
        """
        logger.info(f"Starting game round {game_round} for {game} with Player-1: {player1_model}")
        
        # Initialize agents
        player0_agent = self.get_agent(self.player0_model, self.player0_port)
        player1_agent = self.get_agent(player1_model, self.player1_port)
        
        # Initialize environment
        env = ta.make(env_id=game)
        env = ta.wrappers.LLMObservationWrapper(env=env)
        env = ta.wrappers.SimpleRenderWrapper(
            env=env,
            player_names={0: "Player0", 1: "Player1"},
        )
        
        # Get game summary (Learning from Concept)
        if game not in self.game_summaries:
            self.game_summaries[game] = self.get_game_summary(game, player0_agent, env)
        
        game_summary = self.game_summaries[game]
        
        # Apply learning if enabled and we have previous experiences
        if learning_enabled and game_round > 1 and game_experiences:
            logger.info(f"Applying learning from {len(game_experiences)} previous experiences")
            
            # Learning from Experience: Select top experiences
            selected_experiences = self.select_top_experiences(game_experiences)
            experience_analysis = self.generate_experience_analysis(player1_agent, selected_experiences)
            
            # Learning from Instructor: Get instructor feedback from selected experiences
            instructor_feedback = self.combine_instructor_feedback(player0_agent, selected_experiences)
            
            # Combine all learning sources in Player-1's system prompt
            original_prompt = player1_agent.system_prompt
            
            learning_prompt = ""
            
            # Add Learning from Concept
            if game_summary and not "Error" in game_summary:
                learning_prompt += f"GAME ANALYSIS AND WINNING STRATEGIES:\n{game_summary}\n\n"
            
            # Add Learning from Instructor
            if instructor_feedback:
                learning_prompt += f"INSTRUCTOR FEEDBACK:\n{instructor_feedback}\n\n"
            
            # Add Learning from Experience
            if experience_analysis:
                learning_prompt += f"EXPERIENCE ANALYSIS:\n{experience_analysis}\n\n"
            
            if learning_prompt:
                learning_prompt += "REMEMBER: Apply these strategic principles and insights consistently to maximize your chances of winning.\n\n"
                player1_agent.system_prompt = learning_prompt + original_prompt
            
            # Track selected experiences for debugging
            selected_experience_info = [
                {
                    "round": exp.get("round", "Unknown"),
                    "outcome": exp.get("outcome", "Unknown"),
                    "score": exp.get("score", 0)
                }
                for exp in selected_experiences
            ]
        else:
            # For first game or when learning is disabled, only add game summary
            selected_experience_info = []
            if game_summary and not "Error" in game_summary:
                original_prompt = player1_agent.system_prompt
                concept_prompt = f"GAME ANALYSIS AND WINNING STRATEGIES:\n{game_summary}\n\nREMEMBER: Apply these strategic principles consistently to maximize your chances of winning.\n\n"
                player1_agent.system_prompt = concept_prompt + original_prompt
        
        # Play the game
        agents = {0: player0_agent, 1: player1_agent}
        
        env.reset(num_players=len(agents))
        game_history = {
            "moves": [],
            "outcome": None
        }
        
        done = False
        move_count = 0
        
        try:
            while not done:
                player_id, observation = env.get_observation()
                logger.debug(f"Player {player_id}'s turn - Move {move_count + 1}")
                
                action = agents[player_id](observation)
                logger.debug(f"Player {player_id} action: {action}")
                
                done, info = env.step(action=action)
                
                game_history["moves"].append({
                    "player": player_id,
                    "observation": observation,
                    "action": action
                })
                move_count += 1
            
            rewards = env.close()
            logger.info(f"Game {game_round} completed. Rewards: Player0={rewards[0]}, Player1={rewards[1]}")
            
            # Determine game outcome
            if rewards[0] > rewards[1]:
                game_history["outcome"] = "Player 0 won"
                player1_won = False
            elif rewards[1] > rewards[0]:
                game_history["outcome"] = "Player 1 won"
                player1_won = True
            else:
                game_history["outcome"] = "Draw"
                player1_won = False
            
            # Generate instructor feedback (Learning from Instructor)
            instructor_feedback = self.generate_instructor_feedback(
                player0_agent, game_history, game_history["outcome"]
            )
            
            # Score the game quality
            game_score = self.score_game_quality(player0_agent, game_history, game_history["outcome"])
            
            result = {
                "game": game,
                "round": game_round,
                "player0_model": self.player0_model,
                "player1_model": player1_model,
                "learning_enabled": learning_enabled,
                "rewards": rewards,
                "outcome": game_history["outcome"],
                "player1_won": player1_won,
                "moves": game_history["moves"],
                "instructor_feedback": instructor_feedback,
                "score": game_score,
                "game_summary": game_summary,
                "error": None,
                "selected_experience_info": selected_experience_info
            }
            
            logger.info(f"Game {game_round} outcome: {game_history['outcome']}, Score: {game_score}/10")
            return result
            
        except Exception as e:
            logger.error(f"Error during game {game_round}: {str(e)}")
            return {
                "game": game,
                "round": game_round,
                "player0_model": self.player0_model,
                "player1_model": player1_model,
                "learning_enabled": learning_enabled,
                "rewards": [0, 0],
                "outcome": "Error",
                "player1_won": False,
                "moves": game_history["moves"],
                "instructor_feedback": None,
                "score": 0,
                "game_summary": game_summary,
                "error": str(e),
                "selected_experience_info": []
            }
    
    def run_benchmark(self, games: List[str], player1_model: str, output_file: str, 
                     num_rounds: int = 20, learning_enabled: bool = True) -> List[Dict]:
        """
        Run the complete LearnArena benchmark for a given model.
        
        Args:
            games: List of game environment IDs
            player1_model: Model name for Player-1
            output_file: Path to save results
            num_rounds: Number of rounds per game
            learning_enabled: Whether to enable learning across rounds
        """
        logger.info(f"Starting LearnArena benchmark for {player1_model}")
        logger.info(f"Games: {games}")
        logger.info(f"Rounds per game: {num_rounds}")
        logger.info(f"Learning enabled: {learning_enabled}")
        
        all_results = []
        
        for game in games:
            logger.info(f"Starting benchmark for game: {game}")
            game_experiences = []  # Store experiences for this game
            game_results = []
            
            for round_num in range(1, num_rounds + 1):
                # Run single game
                result = self.run_single_game(
                    game=game,
                    player1_model=player1_model,
                    game_round=round_num,
                    game_experiences=game_experiences,
                    learning_enabled=learning_enabled
                )
                
                game_results.append(result)
                all_results.append(result)
                
                # Add to experiences for learning (exclude current round from future learning)
                if result["error"] is None:
                    experience = {
                        "round": round_num,
                        "outcome": result["outcome"],
                        "score": result["score"],
                        "moves": result["moves"],
                        "instructor_feedback": result["instructor_feedback"]
                    }
                    game_experiences.append(experience)
                
                # Save results after each game
                with open(output_file, 'w') as f:
                    json.dump(all_results, f, indent=2)
                
                logger.info(f"Completed round {round_num}/{num_rounds} for {game}")
            
            # Calculate game statistics
            wins = sum(1 for r in game_results if r["player1_won"])
            win_rate = wins / num_rounds if num_rounds > 0 else 0
            avg_score = sum(r["score"] for r in game_results) / num_rounds if num_rounds > 0 else 0
            
            logger.info(f"Game {game} completed:")
            logger.info(f"  Win rate: {win_rate:.2%} ({wins}/{num_rounds})")
            logger.info(f"  Average score: {avg_score:.1f}/10")
        
        logger.info("LearnArena benchmark completed")
        return all_results

def parse_games_input(games_str: str) -> List[str]:
    """Parse comma-separated games string into a list of games"""
    if not games_str:
        raise ValueError("Games input cannot be empty")
    
    games = [game.strip() for game in games_str.split(',')]
    games = [game for game in games if game]
    
    if not games:
        raise ValueError("No valid games found in input")
        
    return games

def main():
    parser = argparse.ArgumentParser(description="LearnArena: Benchmark for General Learning Ability")
    parser.add_argument("--player0-model", type=str, default="qwen2.5-32b-chat", help="Model name for Player-0 (instructor)")
    parser.add_argument("--player0-path", type=str, required=True, help="Path to Player-0 model")
    parser.add_argument("--player1-model", type=str, required=True, help="Model name for Player-1 (evaluated model)")
    parser.add_argument("--player1-path", type=str, required=True, help="Path to Player-1 model")
    parser.add_argument("--games", type=str, required=True, help="Comma-separated list of games to evaluate")
    parser.add_argument("--output-file", type=str, required=True, help="Output JSON file path")
    parser.add_argument("--num-rounds", type=int, default=20, help="Number of rounds per game")
    parser.add_argument("--gpu", type=int, default=4, help="Number of GPUs to use")
    args = parser.parse_args()
    
    # Parse games
    try:
        games = parse_games_input(args.games)
        logger.info(f"Parsed games: {games}")
    except ValueError as e:
        logger.error(f"Error parsing games input: {str(e)}")
        sys.exit(1)
    
    # Create output directory
    output_dir = os.path.dirname(args.output_file)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Start vLLM servers
    server_processes = []
    
    # Start Player-0 server (Qwen2.5-32B instructor)
    logger.info(f"Starting vLLM server for Player-0 ({args.player0_model}) at {args.player0_path}...")
    proc0 = start_vllm_server(
        model_path=args.player0_path,
        model_name=args.player0_model,
        port=8000,
        gpu=args.gpu
    )
    server_processes.append(proc0)
    
    # Start Player-1 server (evaluated model)
    logger.info(f"Starting vLLM server for Player-1 ({args.player1_model}) at {args.player1_path}...")
    proc1 = start_vllm_server(
        model_path=args.player1_path,
        model_name=args.player1_model,
        port=8001,
        gpu=args.gpu
    )
    server_processes.append(proc1)
    
    try:
        # Initialize benchmark
        benchmark = LearnArenaBenchmark(
            player0_model=args.player0_model,
            player0_port=8000,
            player1_port=8001
        )
        
        # Run benchmark
        results = benchmark.run_benchmark(
            games=games,
            player1_model=args.player1_model,
            output_file=args.output_file,
            num_rounds=args.num_rounds,
            learning_enabled=True
        )
        
        # Print summary
        print("\nLearnArena Benchmark Results Summary:")
        print("=" * 80)
        
        # Group results by game
        games_results = {}
        for result in results:
            game = result['game']
            if game not in games_results:
                games_results[game] = []
            games_results[game].append(result)
        
        total_wins = 0
        total_games = 0
        total_score = 0
        
        for game, game_results in games_results.items():
            wins = sum(1 for r in game_results if r["player1_won"])
            win_rate = wins / len(game_results) if game_results else 0
            avg_score = sum(r["score"] for r in game_results) / len(game_results) if game_results else 0
            
            print(f"\nGame: {game}")
            print(f"Player-0 Model: {args.player0_model}")
            print(f"Player-1 Model: {args.player1_model}")
            print(f"Win Rate: {win_rate:.2%} ({wins}/{len(game_results)})")
            print(f"Average Score: {avg_score:.1f}/10")
            print("-" * 40)
            
            total_wins += wins
            total_games += len(game_results)
            total_score += avg_score * len(game_results)
        
        # Overall statistics
        overall_win_rate = total_wins / total_games if total_games > 0 else 0
        overall_avg_score = total_score / total_games if total_games > 0 else 0
        
        print(f"\nOVERALL PERFORMANCE:")
        print(f"Total Win Rate: {overall_win_rate:.2%} ({total_wins}/{total_games})")
        print(f"Overall Average Score: {overall_avg_score:.1f}/10")
        print(f"Results saved to: {args.output_file}")
        
    finally:
        # Stop all vLLM servers
        for proc in server_processes:
            stop_vllm_server(proc)

if __name__ == "__main__":
    main() 