#!/bin/bash

# LearnArena Benchmark Script
# ===========================
# This script runs the LearnArena benchmark to evaluate general learning ability
# across three learning dimensions:
# 1. Learning from Instructor: Player-0 provides suggestions for improvement
# 2. Learning from Concept: Concise game rule summaries generated by Qwen2.5-32B  
# 3. Learning from Experience: Player-1 analyzes past experiences and applies conclusions

# Default configuration
GAMES="TicTacToe-v0,Checkers-v0,Stratego-v0,TruthAndDeception-v0,SpellingBee-v0,SpiteAndMalice-v0,Tak-v0,WordChains-v0"
OUTPUT_DIR="learnarena_results"
NUM_ROUNDS=20
GPU=8

# Fixed Player-0 configuration (Instructor/Judge)
PLAYER0_MODEL="qwen2.5-32b-chat"
PLAYER0_PATH="/path/to/qwen2.5-32b"

# Player-1 model configurations (Models to be evaluated)
declare -A PLAYER1_CONFIGS=(
    ["qwen2.5-1.5b"]="/path/to/qwen2.5-1.5b"
    ["qwen2.5-7b"]="/path/to/qwen2.5-7b"
    ["qwen2.5-14b"]="/path/to/qwen2.5-14b"
    ["qwen2.5-32b"]="/path/to/qwen2.5-32b"
)

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --games)
            GAMES="$2"
            shift 2
            ;;
        --output-dir)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        --num-rounds)
            NUM_ROUNDS="$2"
            shift 2
            ;;
        --gpu)
            GPU="$2"
            shift 2
            ;;
        --player0-path)
            PLAYER0_PATH="$2"
            shift 2
            ;;
        --help|-h)
            echo "LearnArena Benchmark Script"
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --games GAMES          Comma-separated list of games (default: $GAMES)"
            echo "  --output-dir DIR       Output directory (default: $OUTPUT_DIR)"
            echo "  --num-rounds N         Number of rounds per game (default: $NUM_ROUNDS)"
            echo "  --gpu N                Number of GPUs to use (default: $GPU)"
            echo "  --player0-path PATH    Path to Player-0 model (default: $PLAYER0_PATH)"
            echo "  --help, -h             Show this help message"
            echo ""
            echo "Example:"
            echo "  $0 --games TicTacToe-v0,Checkers-v0,Stratego-v0,TruthAndDeception-v0,SpellingBee-v0,SpiteAndMalice-v0,Tak-v0,WordChains-v0 --num-rounds 20 --gpu 8"
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

# Function to print colored output
print_header() {
    echo -e "\n\033[1;34m========================================\033[0m"
    echo -e "\033[1;34m$1\033[0m"
    echo -e "\033[1;34m========================================\033[0m"
}

print_info() {
    echo -e "\033[1;32m[INFO]\033[0m $1"
}

print_warning() {
    echo -e "\033[1;33m[WARNING]\033[0m $1"
}

print_error() {
    echo -e "\033[1;31m[ERROR]\033[0m $1"
}

# Create output directory if it doesn't exist
mkdir -p "$OUTPUT_DIR"
mkdir -p logs

# Print configuration
print_header "LearnArena Benchmark Configuration"
echo "Games: $GAMES"
echo "Output Directory: $OUTPUT_DIR"
echo "Number of Rounds: $NUM_ROUNDS"
echo "GPU Count: $GPU"
echo "Player-0 Model: $PLAYER0_MODEL"
echo "Player-0 Path: $PLAYER0_PATH"
echo ""
echo "Player-1 Models to evaluate:"
for model_name in "${!PLAYER1_CONFIGS[@]}"; do
    echo "  - $model_name: ${PLAYER1_CONFIGS[$model_name]}"
done

# Verify Player-0 model path exists
if [ ! -d "$PLAYER0_PATH" ] && [ ! -f "$PLAYER0_PATH" ]; then
    print_error "Player-0 model path does not exist: $PLAYER0_PATH"
    print_info "Please update PLAYER0_PATH or use --player0-path argument"
    exit 1
fi

# Verify Python script exists
SCRIPT_PATH="learnarena_benchmark.py"
if [ ! -f "$SCRIPT_PATH" ]; then
    print_error "LearnArena benchmark script not found: $SCRIPT_PATH"
    exit 1
fi

# Function to run benchmark for a single model
run_model_benchmark() {
    local model_name=$1
    local model_path=$2
    
    print_header "Running LearnArena Benchmark: $model_name"
    
    # Set output file for this model
    local output_file="$OUTPUT_DIR/learnarena_${model_name}_results.json"
    
    print_info "Starting benchmark for $model_name"
    print_info "Model path: $model_path"
    print_info "Output file: $output_file"
    
    # Run the benchmark
    python $SCRIPT_PATH \
        --player0-model "$PLAYER0_MODEL" \
        --player0-path "$PLAYER0_PATH" \
        --player1-model "$model_name" \
        --player1-path "$model_path" \
        --games "$GAMES" \
        --output-file "$output_file" \
        --num-rounds "$NUM_ROUNDS" \
        --gpu "$GPU"
    
    local exit_code=$?
    
    if [ $exit_code -eq 0 ]; then
        print_info "Successfully completed benchmark for $model_name"
        print_info "Results saved to $output_file"
        
        # Generate quick summary
        if [ -f "$output_file" ]; then
            print_info "Quick Summary for $model_name:"
            python3 -c "
import json
try:
    with open('$output_file', 'r') as f:
        results = json.load(f)
    
    # Group by game
    games_stats = {}
    for result in results:
        game = result['game']
        if game not in games_stats:
            games_stats[game] = {'wins': 0, 'total': 0, 'scores': []}
        
        games_stats[game]['total'] += 1
        if result['player1_won']:
            games_stats[game]['wins'] += 1
        games_stats[game]['scores'].append(result['score'])
    
    # Print summary
    total_wins = 0
    total_games = 0
    total_score = 0
    
    for game, stats in games_stats.items():
        win_rate = stats['wins'] / stats['total'] if stats['total'] > 0 else 0
        avg_score = sum(stats['scores']) / len(stats['scores']) if stats['scores'] else 0
        print(f'  {game}: {win_rate:.1%} win rate ({stats[\"wins\"]}/{stats[\"total\"]}), {avg_score:.1f}/10 avg score')
        
        total_wins += stats['wins']
        total_games += stats['total']
        total_score += sum(stats['scores'])
    
    overall_win_rate = total_wins / total_games if total_games > 0 else 0
    overall_avg_score = total_score / total_games if total_games > 0 else 0
    print(f'  OVERALL: {overall_win_rate:.1%} win rate ({total_wins}/{total_games}), {overall_avg_score:.1f}/10 avg score')
    
except Exception as e:
    print(f'Error generating summary: {e}')
"
        fi
    else
        print_error "Benchmark failed for $model_name (exit code: $exit_code)"
        return 1
    fi
    
    echo ""
    return 0
}

# Main execution
print_header "Starting LearnArena Benchmark Suite"

# Track results
declare -a successful_models=()
declare -a failed_models=()

# Run benchmark for each Player-1 model
for model_name in "${!PLAYER1_CONFIGS[@]}"; do
    model_path="${PLAYER1_CONFIGS[$model_name]}"
    
    # Check if model path exists
    if [ ! -d "$model_path" ] && [ ! -f "$model_path" ]; then
        print_warning "Skipping $model_name: model path does not exist ($model_path)"
        failed_models+=("$model_name (path not found)")
        continue
    fi
    
    # Run benchmark
    if run_model_benchmark "$model_name" "$model_path"; then
        successful_models+=("$model_name")
    else
        failed_models+=("$model_name")
    fi
done

# Print final summary
print_header "LearnArena Benchmark Suite Summary"

echo "Successful benchmarks (${#successful_models[@]}):"
for model in "${successful_models[@]}"; do
    echo "  ✓ $model"
done

if [ ${#failed_models[@]} -gt 0 ]; then
    echo ""
    echo "Failed benchmarks (${#failed_models[@]}):"
    for model in "${failed_models[@]}"; do
        echo "  ✗ $model"
    done
fi

# Generate comprehensive results summary
print_header "Comprehensive Results Analysis"

if [ ${#successful_models[@]} -gt 0 ]; then
    print_info "Generating comprehensive analysis..."
    
    # Create a summary script
    cat > "$OUTPUT_DIR/generate_summary.py" << 'EOF'
import json
import os
import sys
from collections import defaultdict

def analyze_results(output_dir):
    results_files = [f for f in os.listdir(output_dir) if f.startswith('learnarena_') and f.endswith('_results.json')]
    
    if not results_files:
        print("No result files found.")
        return
    
    all_model_results = {}
    
    for file in results_files:
        model_name = file.replace('learnarena_', '').replace('_results.json', '')
        file_path = os.path.join(output_dir, file)
        
        try:
            with open(file_path, 'r') as f:
                results = json.load(f)
            
            model_stats = defaultdict(lambda: {'wins': 0, 'total': 0, 'scores': []})
            
            for result in results:
                game = result['game']
                model_stats[game]['total'] += 1
                if result['player1_won']:
                    model_stats[game]['wins'] += 1
                model_stats[game]['scores'].append(result['score'])
            
            all_model_results[model_name] = dict(model_stats)
            
        except Exception as e:
            print(f"Error processing {file}: {e}")
    
    # Print comparison table
    print("\nLearnArena Benchmark Results Comparison")
    print("=" * 80)
    
    # Get all games
    all_games = set()
    for model_results in all_model_results.values():
        all_games.update(model_results.keys())
    all_games = sorted(all_games)
    
    # Print header
    print(f"{'Model':<20} {'Overall':<12} ", end="")
    for game in all_games:
        print(f"{game:<15}", end="")
    print()
    
    print("-" * (32 + len(all_games) * 15))
    
    # Print results for each model
    for model_name, model_results in sorted(all_model_results.items()):
        total_wins = sum(stats['wins'] for stats in model_results.values())
        total_games = sum(stats['total'] for stats in model_results.values())
        total_score = sum(sum(stats['scores']) for stats in model_results.values())
        
        overall_win_rate = total_wins / total_games if total_games > 0 else 0
        overall_avg_score = total_score / total_games if total_games > 0 else 0
        
        print(f"{model_name:<20} {overall_win_rate:.1%} ({overall_avg_score:.1f}/10) ", end="")
        
        for game in all_games:
            if game in model_results:
                stats = model_results[game]
                win_rate = stats['wins'] / stats['total'] if stats['total'] > 0 else 0
                avg_score = sum(stats['scores']) / len(stats['scores']) if stats['scores'] else 0
                print(f"{win_rate:.1%} ({avg_score:.1f})<5}", end=" ")
            else:
                print(f"{'N/A':<15}", end="")
        print()
    
    print("\nFormat: Win Rate (Average Score/10)")
    print("Legend: Higher win rates and scores indicate better performance")

if __name__ == "__main__":
    output_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    analyze_results(output_dir)
EOF

    python3 "$OUTPUT_DIR/generate_summary.py" "$OUTPUT_DIR"
    rm "$OUTPUT_DIR/generate_summary.py"
else
    print_warning "No successful benchmarks to analyze"
fi

# Final status
echo ""
if [ ${#failed_models[@]} -eq 0 ]; then
    print_info "All benchmarks completed successfully!"
    exit 0
else
    print_warning "Some benchmarks failed. Check the logs for details."
    exit 1
fi 